{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "Get unique IDs from a flat file.\n",
    "\n",
    "### Restrictions\n",
    "\n",
    "1. The program can only run in a single machine, not a distribution system\n",
    "\n",
    "2. The flat file is too big to fit in the memory\n",
    "\n",
    "3. The unique IDs are also too big to fit in the memory.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- Characters in these IDs: only alpha and numeric character a-z|A-Z|0-9, which is 62 chars in total. The reason is many uniq ID generating system will use these common chars to produced an ID.\n",
    "\n",
    "- Length of a single ID: It doesn’t matter much as the length of IDs are not vary too much, and each ID could fix in the memory. :) Let’s assume the length would be 10 chars for simplicity.\n",
    "\n",
    "  With the two assumptions, the total number of uniqe ID is $62^7 = 3,521,614,606,208$. Let’s say 1% of these numbers are in the final unique ID list.  It will take 246GB  ($62^7 / 100 * 7B$) space, which cannot fit in most computer’s RAM.\n",
    "\n",
    "- The original file is around 50GB (~7 billioin lines),  and my computer has 8GB RAM memory and 1TB disk space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare a input ID file for demostration purpose: input_id.txt\n",
    "\n",
    "import random\n",
    "\n",
    "def gen_ids(valid_chars, length, N):\n",
    "    \"\"\"Generate N number of id:\n",
    "    - each char of the id is in valid_chars\n",
    "    - the lenght of id is length\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    for n in range(N):\n",
    "        id = random.choices(valid_chars, k=length)\n",
    "        ids.append(''.join(id))\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 'abcdefghigklmnopqrstuvwxyz'\n",
    "nums = '01234567890'\n",
    "CHARS = alpha + alpha.upper() + nums\n",
    "\n",
    "file_name = 'input_id.txt' # file for generated IDs\n",
    "length = 7\n",
    "N = 10000 # 10 kilo, could be 7 billion :)\n",
    "\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write('\\n'.join(gen_ids(CHARS, length, N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2PlyIKp',\n",
       " 'PD00Lb4',\n",
       " 'enQ1kYy',\n",
       " 'bVGIGTn',\n",
       " 'b1XmYDq',\n",
       " 'KZeXgWC',\n",
       " 'vZvc2yT',\n",
       " 'Sh28z05',\n",
       " 'yC0Lklb',\n",
       " 'SAr7D4w']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_ids(CHARS, 7, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Count the ID starting with a, b, c...\n",
    "\n",
    "Count the number of unique IDs start each char of a-zA-Z0-9, and then append them in a file. Thus, this file contains all the unique IDs of the input file. \n",
    "\n",
    "This approach assumes all IDs starting with any char could fit in the memory as there is no hotspot. We can do this by sampling the input file (1 million lines, 7MB size) and check the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: shuf: command not found\n"
     ]
    }
   ],
   "source": [
    "# Here is the command in **Linux** (not working in macOS) to get the ramdon n lines from input file and save it to the sample file.\n",
    "\n",
    "!shuf -n 1000000 input_id.txt > sample_id.txt\n",
    "\n",
    "# Note: This command doesn't work in macOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w 176\n",
      "I 163\n",
      "N 172\n",
      "r 157\n",
      "H 175\n",
      "c 158\n",
      "2 150\n",
      "4 184\n",
      "E 161\n",
      "0 316\n",
      "D 161\n",
      "F 159\n",
      "f 139\n",
      "u 158\n",
      "W 167\n",
      "y 163\n",
      "S 160\n",
      "h 170\n",
      "3 161\n",
      "A 157\n",
      "B 157\n",
      "q 146\n",
      "g 307\n",
      "l 174\n",
      "Y 134\n",
      "T 168\n",
      "6 170\n",
      "p 151\n",
      "x 157\n",
      "k 147\n",
      "d 157\n",
      "M 150\n",
      "G 296\n",
      "8 170\n",
      "O 133\n",
      "9 178\n",
      "Z 168\n",
      "s 144\n",
      "V 178\n",
      "e 164\n",
      "b 145\n",
      "5 161\n",
      "i 147\n",
      "n 144\n",
      "R 171\n",
      "X 158\n",
      "o 148\n",
      "U 158\n",
      "z 171\n",
      "t 164\n",
      "v 170\n",
      "1 156\n",
      "7 145\n",
      "K 180\n",
      "a 151\n",
      "m 167\n",
      "C 145\n",
      "Q 134\n",
      "L 170\n",
      "P 159\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of the first char (a-zA-Z0-9) of input IDs\n",
    "\n",
    "import fileinput\n",
    "from collections import defaultdict\n",
    "\n",
    "counter = defaultdict(int)\n",
    "with fileinput.input(files='sample_id.txt') as f:    \n",
    "    for i in f:\n",
    "        counter[i[:1]] += 1\n",
    "\n",
    "for key, value in counter.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder for splited id files\n",
    "! mkdir splits\n",
    "\n",
    "# split the big file to small pieces (70 million lines, ~500 MB) using linux command: split\n",
    "! split -l 70000000 input_id.txt splits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fileinput\n",
    "import collections\n",
    "\n",
    "# uniq_file = 'unique_id.txt'\n",
    "\n",
    "#Return a set of IDs started with `start_char` in file `file`\n",
    "def extract_uniq_ids(file, start_char):\n",
    "    uniq_ids = set() # id start with 'start_char'\n",
    "    with fileinput.input(files = file) as lines:\n",
    "        for line in lines:\n",
    "            if line.startswith(start_char) and line not in uniq_ids:\n",
    "                uniq_ids.add(line.strip())\n",
    "    return uniq_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'id-'\n",
    "\n",
    "for root, dirs, files in os.walk(\"splits\"):\n",
    "    for start_char in CHARS:\n",
    "        uniq_ids = set()\n",
    "        for file_name in files:\n",
    "            if file_name.startswith('.'):\n",
    "                continue # bugfix: skip the hidden files from OS\n",
    "            \n",
    "            file = os.sep.join((root, file_name))\n",
    "            uniq_ids = uniq_ids.union(extract_uniq_ids(file, start_char))\n",
    "        \n",
    "        if start_char.isupper(): # bugfix for case insensitive OS like mac\n",
    "            start_char += '_'\n",
    "            \n",
    "        if uniq_ids:\n",
    "            with open(prefix + start_char + '.txt', 'w') as f:\n",
    "                f.write('\\n'.join(uniq_ids))\n",
    "                f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10000 uniq_id.txt\n"
     ]
    }
   ],
   "source": [
    "# simple merge these small files into a big one, the number should be equal the uniq id in the orignal file.\n",
    "\n",
    "!cat id-*.txt > uniq_id.txt\n",
    "!wc -l uniq_id.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10000\n"
     ]
    }
   ],
   "source": [
    "# count the uniq numer of id using linux command to verify.\n",
    "\n",
    "!sort uniq_id.txt | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number matches. 10000 records is too small to get duplicated ID. Try 7 billion records will be anohter history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unit Test\n",
    "\n",
    "Clean the folder, and put this file as 'unit_test.txt' in the splits folder and run the above programs.\n",
    "```\n",
    "xNDGN3R\n",
    "8guP0Af\n",
    "VHgwqgA\n",
    "Wy0AXoo\n",
    "wy0AXoo\n",
    "xNDGN3R\n",
    "toS3f6T\n",
    "8bIgIXy\n",
    "xNDGN3R\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort: No such file or directory\n",
      "       0\n"
     ]
    }
   ],
   "source": [
    "# Then use linux command cat, sort, uniq to merge these files and count the number of uniq files.\n",
    "!sort splits/unit_test.txt | uniq | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So the final file will be 'uniq_id.txt'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation and Bottleneck\n",
    "\n",
    "If the IDs are highly skewed (huge ID starts with the same char), then this method won't work because those group of IDs cannot fix in memory.\n",
    "\n",
    "The bottleneck of this program is the multiple I/O read, becuase it will read the splitted files 62 times as each start char counts once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "\n",
    "To reduce the heavy I/O, and avoid the hotsopt/skew issues:\n",
    "1. Read one split file each time.\n",
    "2. Get all the IDs in a map, the key of which is the start char such as a, b, c... 7, 8, 9, and the value of which is a set of IDs\n",
    "3. For each <key, value> in step 2, update the coresponding output file by merge the IDs to the existing ones by:\n",
    "   Read the existing/new smaill id-x file in a set;\n",
    "   Merge the set with the value set in the map of step 2\n",
    "   Flush the merged set to the disk\n",
    "\n",
    "Here is the implementation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'id-'\n",
    "CHARS_SET = set(CHARS) # for quick lookup\n",
    "\n",
    "import fileinput\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_ids(file):\n",
    "    uniq_ids = defaultdict(set) # k:'[a-z|A-Z|0-9]', value = set()\n",
    "    with fileinput.input(files=file) as f:\n",
    "        for line in f:\n",
    "            first_char = line[:1]\n",
    "            if first_char in CHARS_SET:\n",
    "                uniq_ids[first_char].add(line.strip())\n",
    "    return uniq_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'x': {'xNDGN3R'},\n",
       "             '8': {'8bIgIXy', '8guP0Af'},\n",
       "             'V': {'VHgwqgA'},\n",
       "             'W': {'Wy0AXoo'},\n",
       "             'w': {'wy0AXoo'},\n",
       "             't': {'toS3f6T'}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ids('splits/unit_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "def merge_ids(file, other_ids):\n",
    "    ids = set()\n",
    "    \n",
    "    if os.path.isfile(file): \n",
    "        with open(file, 'r') as f:\n",
    "            ids = set(line.strip() for line in f.readlines())\n",
    "            f.close()\n",
    "    \n",
    "    ids = ids.union(set(other_ids))\n",
    "    \n",
    "    with open(file, 'w') as f:\n",
    "        f.write('\\n'.join(ids))\n",
    "        f.write('\\n')\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"splits\"):\n",
    "    for file_name in files:\n",
    "        if file_name.startswith('.'):\n",
    "            continue # bugfix: skip the hidden files from OS\n",
    "\n",
    "        file = os.sep.join((root, file_name))\n",
    "        for start_char, id_set in extract_ids(file).items():\n",
    "            if start_char.isupper(): # bugfix for case insensitive OS like mac\n",
    "                start_char += '_'\n",
    "            file_to_merge = prefix + start_char + '.txt'\n",
    "            merge_ids(file_to_merge, id_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Use database column's unique feature\n",
    "\n",
    "**Intution**\n",
    "1. Take the same step as before to split the files into small files.\n",
    "2. Create a table `unique_id` with one column `id` which is unique in MySQL.\n",
    "3. Take one files and read all the IDs, then batch insert these values into the database using `insert ignore`.\n",
    "4. Continue to process all the splitted files till the end.\n",
    "\n",
    "The reason why it works is because the duplicated values are ignored. Here is the SQL code example for reference. In order to make it work, I could use python to read the splitted file in a set (not very necessary), and then `insert ignore` these data to the table `unique_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "create table unique_id (id char(7) UNIQUE not NULL);\n",
    "\n",
    "insert ignore into unique_id VALUES\n",
    "('xNDGN3R'),\n",
    "('8guP0Af'),\n",
    "('VHgwqgA'),\n",
    "('Wy0AXoo'),\n",
    "('wy0AXoo'),\n",
    "('xNDGN3R'),\n",
    "('toS3f6T'),\n",
    "('8bIgIXy'),\n",
    "('xNDGN3R');\n",
    "\n",
    "select * from unique_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verification**\n",
    "select * from unique_id;\n",
    "\n",
    "```\n",
    "id\n",
    "---\n",
    "8bIgIXy\n",
    "8guP0Af\n",
    "toS3f6T\n",
    "VHgwqgA\n",
    "Wy0AXoo\n",
    "xNDGN3R\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
